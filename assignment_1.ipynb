{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustainable and Entrepreneurial Finance\n",
    "\n",
    "### Assignment 1 - Portfolio allocation\n",
    "\n",
    "#### Group 8 - Energy Firms With Available Scope 1 to 3 emissions (TRUCOST)\n",
    "\n",
    "Useful imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import prettytable\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0 - Importing and preparing datasets for calculation.\n",
    "\n",
    "Importing the files and creating the raw pandas data frames (it might take a while...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path names\n",
    "\n",
    "github_path = 'https://github.com/percw/Sustainable_and_Entrepreneurial_Finance/blob/master/Data_Excel'\n",
    "\n",
    "path_gics = f'{github_path}/Trucost_CO2emissions/GICS_map%202018.xlsx?raw=true'\n",
    "path_sector = f'{github_path}/Trucost_CO2emissions/sector.xlsx?raw=True'\n",
    "path_returns = f'{github_path}/MSCI_ESGscores/Returns/monthlyreturns.xlsx?raw=True'\n",
    "path_caps = f'{github_path}/MSCI_ESGscores/Fundamentals/size.xlsx?raw=True'\n",
    "\n",
    "# Scope paths\n",
    "path_scope1 = f'{github_path}/Trucost_CO2emissions/scope1.xlsx?raw=true'\n",
    "path_scope2 = f'{github_path}/Trucost_CO2emissions/scope2.xlsx?raw=true'\n",
    "path_scope3 = f'{github_path}/Trucost_CO2emissions/scope3.xlsx?raw=true'\n",
    "\n",
    "# Local paths in case of very slow loading...\n",
    "#path_gics = './Data_Excel/Trucost_CO2emissions/GICS_map 2018.xlsx'\n",
    "#path_sector = './Data_Excel/Trucost_CO2emissions/sector.xlsx'\n",
    "#path_returns = './Data_Excel/MSCI_ESGscores/Returns/monthlyreturns.xlsx'\n",
    "#path_caps = './Data_Excel/MSCI_ESGscores/Fundamentals/size.xlsx'\n",
    "\n",
    "# Reading excel files and creating pandas data frames\n",
    "df_gics_raw = pd.read_excel(path_gics)\n",
    "df_sector_raw = pd.read_excel(path_sector)\n",
    "df_returns_raw = pd.read_excel(path_returns)\n",
    "df_caps_raw = pd.read_excel(path_caps)\n",
    "\n",
    "df_scope1_raw = pd.read_excel(path_scope1)\n",
    "df_scope2_raw = pd.read_excel(path_scope2)\n",
    "df_scope3_raw = pd.read_excel(path_scope3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming and copying the raw dataframes for convenience, so if we need to rerun som code we don't need to wait for the excel files to be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gics = df_gics_raw.copy()\n",
    "df_sector = df_sector_raw.copy()\n",
    "df_returns = df_returns_raw.copy()\n",
    "df_caps = df_caps_raw.copy()\n",
    "df_scope1 = df_scope1_raw.copy()\n",
    "df_scope2 = df_scope2_raw.copy()\n",
    "df_scope3 = df_scope3_raw.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the index for returns and market caps from '`Unnamed: 0`' to '`date`'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming index data column\n",
    "\n",
    "df_returns.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "df_caps.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "display(df_returns)\n",
    "display(df_caps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ISIN codes for the energy companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_code = 1010.0  # based on Global Industry Classification Standard GICS\n",
    "df_energy = df_sector.loc[df_sector['GICSIG'] == industry_code]\n",
    "energy_isin = df_energy['ISIN'].values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the returns for the companies matching the ISIN codes in `energy_isin`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all ISIN codes to iterate through\n",
    "return_cols = df_returns.columns.values.tolist()\n",
    "display(len(return_cols))\n",
    "display(len(return_cols) == df_returns.shape[1])\n",
    "\n",
    "# Creating a list with all the ISIN Energy codes that the returns.xlsx datasheet contains\n",
    "both = []\n",
    "for c in return_cols:\n",
    "    if c in energy_isin:\n",
    "        both.append(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5141 columns, and the same amount of columns in df_returns.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape and general characteristics of our new list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(return_cols[:4])\n",
    "display(energy_isin[:4])\n",
    "display(both[:4])\n",
    "display(len(return_cols))\n",
    "display(len(energy_isin))\n",
    "display(len(both))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually checking that the first four columns and companies to the ones displayed above in `df_returns`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the date column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date' not in energy_isin:\n",
    "    energy_isin.insert(0, 'date')\n",
    "\n",
    "energy_isin[:4]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make sure that the energy companies have Scope 1-3 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_nrg = df_scope1[df_scope1['ISIN'].isin(energy_isin)]\n",
    "scope2_nrg = df_scope2[df_scope2['ISIN'].isin(energy_isin)]\n",
    "scope3_nrg = df_scope3[df_scope3['ISIN'].isin(energy_isin)]\n",
    "display(scope1_nrg.shape)\n",
    "display(scope2_nrg.shape)\n",
    "display(scope3_nrg.shape)\n",
    "scope3_nrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_isin = df_scope1['ISIN'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs in each row\n",
    "n_nulls_1 = scope1_nrg.isna().sum(axis=1)\n",
    "\n",
    "# Filter the rows with 22 NaN values\n",
    "no_scope_1 = scope1_nrg[n_nulls_1 == 22]\n",
    "\n",
    "display(no_scope_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs in each row\n",
    "n_nulls_2 = scope2_nrg.isna().sum(axis=1)\n",
    "\n",
    "# Filter the rows with 22 NaN values\n",
    "no_scope_2 = scope2_nrg[n_nulls_2 == 22]\n",
    "\n",
    "display(no_scope_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs in each row\n",
    "n_nulls_3 = scope3_nrg.isna().sum(axis=1)\n",
    "\n",
    "# Filter the rows with 22 NaN values\n",
    "no_scope_3 = scope3_nrg[n_nulls_3 == 22]\n",
    "\n",
    "display(no_scope_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all firms in these datasets have at least one value for each scope 1, 2, and 3. Now we must make sure that the ISINs we work with are part of these ISINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_nrg_isin = scope1_nrg['ISIN'].values.tolist()\n",
    "scope2_nrg_isin = scope2_nrg['ISIN'].values.tolist()\n",
    "scope3_nrg_isin = scope3_nrg['ISIN'].values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are any differences between the companies in the respective Scope 1-3 list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(scope1_nrg_isin) == set(scope2_nrg_isin):\n",
    "    print(\"scope1_nrg_isin and scope2_nrg_isin have the same elements (order doesn't matter)\")\n",
    "else:\n",
    "    print(\"The two lists are different\")\n",
    "    \n",
    "if set(scope1_nrg_isin) == set(scope3_nrg_isin):\n",
    "    print(\"scope1_nrg_isin and scope3_nrg_isin have the same elements (order doesn't matter)\")\n",
    "else:\n",
    "    print(\"The two lists are different\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All companies in Scope 1 are in Scope 2 and Scope 3. Thats good."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together the return data for the energy companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrg_returns = df_returns[df_returns.columns.intersection(energy_isin)]\n",
    "display(nrg_returns)\n",
    "\n",
    "# Checking the datatypes.\n",
    "display(nrg_returns.dtypes.unique())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks good. We have only float64 values which is as expected and good. Additionally we see that we have 223 columns which is the same as the length of the ISIN list created in the codeblock above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter on the targeted dates which is from 01.01.2005 to 31.12.2020. We'll use a mask to get the observations in this timeframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2005-01-01'\n",
    "end_date = '2020-12-31'\n",
    "\n",
    "# Greater than or equal to the start date and smaller than or equal the end date\n",
    "mask = (nrg_returns['date'] >= start_date) & (nrg_returns['date'] <= end_date)\n",
    "\n",
    "nrg_returns = nrg_returns.loc[mask]\n",
    "\n",
    "display(nrg_returns.iloc[0][0])\n",
    "display(nrg_returns.iloc[-1][0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the first and the last column has the correct dates.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the `NaN` values for companies that have more than 36 months of no return data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all companies that have 36 NAN values (3 years) or more\n",
    "years = 3\n",
    "months = 12\n",
    "too_many_nans = years*months\n",
    "\n",
    "nrg_returns = nrg_returns.dropna(\n",
    "    thresh=len(nrg_returns) - too_many_nans, axis=1)\n",
    "nrg_returns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a copy for Question 3 called `nrg_returns_date_column` without the `date` as index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrg_returns_date_column = nrg_returns.copy()  # For Q3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set `date` as the index column on the nrg_returns dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date' in nrg_returns.columns.values.tolist():\n",
    "    nrg_returns.set_index('date', inplace=True)\n",
    "display(nrg_returns.isnull().sum().sum())\n",
    "display(nrg_returns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next and last check we have to do is to see if there are any difference in the company list of the `nrg_returns` and the `Scope 1-3` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all columns are in scope1_nrg_isin to see if all of them have scope 1 to 3 emissions available\n",
    "# check if all column names of the dataframe are in the list\n",
    "if set(nrg_returns.columns).issubset(scope1_nrg_isin):\n",
    "    print(\"All columns of the dataframe are in the list\")\n",
    "else:\n",
    "    print(\"Not all columns of the dataframe are in the list\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first plot. Let's plot the monthly returns with date on the x-axis and the return rate on the y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot (plot all lines)\n",
    "\n",
    "ax = nrg_returns.plot(linewidth=0.5, figsize=(12, 8))\n",
    "ax.get_legend().remove()  # Removing the legend\n",
    "ax.set_title('All monthly returns')  # Title\n",
    "ax.set_ylabel('Returns')  # y-axis label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we have (some) huge outliers distorting the plot. Let's create a treshold of 1000%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_threshold = 10\n",
    "outlier_companies = nrg_returns.loc[:, nrg_returns[(\n",
    "    nrg_returns > outlier_threshold)].any(axis=0)]\n",
    "outlier_list = outlier_companies.columns.values.tolist()\n",
    "outlier_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only one outlier, thats good. Let's remove it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outlier_list[0] in nrg_returns.columns.values.tolist():\n",
    "    nrg_returns = nrg_returns.drop(columns=outlier_list)\n",
    "\n",
    "nrg_returns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new plot without the outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot (plot all lines)\n",
    "\n",
    "ax = nrg_returns.plot(linewidth=0.5, figsize=(12, 8))\n",
    "ax.get_legend().remove()  # removing the legend\n",
    "ax.set_title('All monthly returns')  # Title\n",
    "ax.set_ylabel('Returns')  # y-axis label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks much better.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to export the new dataframe to a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the current working directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Defining subfolder path\n",
    "path = directory+'/Clean_Data'\n",
    "\n",
    "# Checking whether the specified path already exists - will avoid errors when re-running\n",
    "if os.path.isdir(path):\n",
    "    print(f'Path {path} already exists')\n",
    "    pass\n",
    "\n",
    "# If not, create subfolder\n",
    "else:\n",
    "    os.mkdir('Clean_Data')\n",
    "\n",
    "# Saving the clean merged dataframe as a csv in a subfolder\n",
    "nrg_returns.to_csv('Clean_Data/nrg_returns.csv')\n",
    "# files.download('nrg_returns.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Market Caps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at the Market Cap data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the intersection between the energy companies that are in the the returns dataset (`returns_isin`) and the market caps (`df_caps`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_isin = nrg_returns.columns.values.tolist()\n",
    "nrg_caps = df_caps[df_returns.columns.intersection(returns_isin)]\n",
    "nrg_caps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will for this dataset also set the `date` column as the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date' in nrg_caps.columns.values.tolist():  # date as index\n",
    "    nrg_caps.set_index('date', inplace=True)\n",
    "\n",
    "nrg_returns.isnull().sum().sum()  # Count NANs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start answering the questions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QI - Annual average return and annualized volatility for all individual assets over the period 2005-2020. Correlation between individual average returns and volatility individually and between both metrics.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop through the columns representing energy returns (`nrg_returns`). Using method `.std()` to get standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1 = pd.DataFrame([])\n",
    "\n",
    "for a in nrg_returns.columns.values.tolist():\n",
    "\n",
    "    # Get annualized average return\n",
    "    avg_monthly = nrg_returns[a].mean()\n",
    "    annualized_avg_return = avg_monthly*months\n",
    "\n",
    "    # Get annualized volatility\n",
    "    std_monthly = nrg_returns[a].std()\n",
    "    annualized_volatility = std_monthly*math.sqrt(months)\n",
    "\n",
    "    # Create series\n",
    "    asset = {'AAR': annualized_avg_return, 'volatility': annualized_volatility}\n",
    "    series = pd.Series(data=asset, index=['AAR', 'volatility'])\n",
    "\n",
    "    # Concat\n",
    "    df_q1 = pd.concat([df_q1, series.rename(a)], axis=1)\n",
    "\n",
    "# Transpose df for readability\n",
    "df_q1 = df_q1.T\n",
    "\n",
    "# Show\n",
    "display(df_q1.head())\n",
    "display(df_q1.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look a the descriptive statistics of the newly created dataset and plot the variance and annualized average returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_q1.describe())\n",
    "df_q1_plot = df_q1.copy()\n",
    "df_q1_plot\n",
    "ax = df_q1.reset_index().plot(kind='scatter', x='index',  y='AAR', figsize=(6, 4))\n",
    "ax.set_xlabel('Energy companies')\n",
    "ax.set_title('Annualized average Return for each Energy Company')  # Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_q1.describe())\n",
    "df_q1_plot = df_q1.copy()\n",
    "df_q1_plot\n",
    "ax = df_q1.reset_index().plot(kind='scatter', x='index',  y='volatility', figsize=(6, 4))\n",
    "ax.set_xlabel('Energy companies')\n",
    "ax.set_title('Annualized average Return for each Energy Company')  # Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_q1.describe())\n",
    "ax = df_q1.plot(kind='scatter', x='volatility', y='AAR', figsize=(6, 4))\n",
    "ax.set_title('Annualized average Return against Volatility')  # Title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a plot with the correlation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between individual average returns and volatility\n",
    "corr_arr_vol_q1 = df_q1['AAR'].corr(df_q1['volatility'])\n",
    "corr_arr_vol_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use the function regplot to make a scatterplot\n",
    "sns.regplot(x=df_q1['volatility'], y=df_q1['AAR'], color='#4A90E2', seed=0)\n",
    "\n",
    "# Add a title and axis labels\n",
    "plt.title('Correlation between AAR and Volatility')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('AAR')\n",
    "\n",
    "# Customize the tick marks\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "plt.legend(labels=[f'Correlation: {round(corr_arr_vol_q1, 3)}'])\n",
    "\n",
    "# Change the background color\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our data is correct by manually calculating the AAR and volatility for the company with ISIN: `AU000000ERA9`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrg_returns['AU000000ERA9'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that this is correct by manually doing the same for the 2nd asset\n",
    "\n",
    "# AAR\n",
    "check_aar = nrg_returns['AU000000ERA9'].mean()\n",
    "check_aar_annualized = check_aar*months\n",
    "\n",
    "# Volatility\n",
    "check_std = nrg_returns['AU000000ERA9'].std()\n",
    "check_std_annualized = check_std*math.sqrt(months)\n",
    "\n",
    "rounds = 50\n",
    "\n",
    "display(round(check_aar_annualized, rounds) == round(df_q1.iloc[2][0], rounds))\n",
    "display(round(check_std_annualized, rounds) == round(df_q1.iloc[2][1], rounds))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the values are equal even with 50 decimals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.II - Equally-weighted and value-weighted portfolio with monthly rebalancing over the period 2005-2020. Report the following statistics for both portfolios: annualized average return, annualized volatility, minimum return, maximum return, and Sharpe ratio. Plot the time series of return for both portfolios\n",
    "\n",
    "#### Q.II.I Building dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building melted df with market caps\n",
    "\n",
    "df_q2 = nrg_returns_date_column.drop(columns=['IE00BLNN3691']).copy()\n",
    "df_q2 = df_q2.melt(id_vars=['date'], var_name='ISIN',\n",
    "                   value_name='monthly_return')\n",
    "\n",
    "df_q2['date'] = pd.to_datetime(df_q2['date'], infer_datetime_format=True)\n",
    "df_q2['year'] = df_q2.date.dt.year\n",
    "df_q2['month'] = df_q2.date.dt.month\n",
    "df_q2 = df_q2[['date', 'year', 'month', 'ISIN', 'monthly_return']].copy()\n",
    "\n",
    "df_size = df_caps.melt(\n",
    "    id_vars=['date'], var_name='ISIN', value_name='market_cap')\n",
    "df_q2 = pd.merge(df_q2, df_size, how='left', on=('date', 'ISIN'))\n",
    "\n",
    "df_q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual return (sum of monthly returns per year per ISIN)\n",
    "an_rtrn = df_q2[['year', 'ISIN', 'monthly_return']\n",
    "                ].groupby(['year', 'ISIN']).sum().copy()\n",
    "\n",
    "\n",
    "def annual_return(row):\n",
    "    ISIN = row['ISIN']\n",
    "    year = row['year']\n",
    "    return an_rtrn.loc[(year, ISIN)][0]\n",
    "\n",
    "\n",
    "df_q2['annual_returns'] = df_q2.apply(annual_return, axis=1)\n",
    "df_q2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.III.II Building the Equally Weighted Portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building equally weighted portfolio with monthly rebalancing\n",
    "df_q2_e = df_q2.copy()\n",
    "\n",
    "num_assets = len(df_q2_e.columns.values.tolist())\n",
    "equal_weight = 1 / num_assets\n",
    "df_q2_e.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the AAR of the equally-weighted portfolio for the 15 years under observation\n",
    "AAR = df_q2_e[['ISIN', 'monthly_return']].groupby(\n",
    "    'ISIN').apply(lambda x: x.mean()*months).copy()\n",
    "\n",
    "\n",
    "def AAR_func(row):\n",
    "    ISIN = row['ISIN']\n",
    "    return AAR.loc[(ISIN)][0]\n",
    "\n",
    "\n",
    "df_q2_e['ew_AAR'] = df_q1.AAR.mean()\n",
    "df_q2_e['ew_annualized_volatility'] = df_q2_e.monthly_return.std() * \\\n",
    "    math.sqrt(months)\n",
    "\n",
    "# Computing the annual return of the equally-weighted portolio\n",
    "eq_weight_port_df = df_q2_e[['year', 'annual_returns']].groupby('year').mean()\n",
    "\n",
    "\n",
    "def eq_weight_port_func(row):\n",
    "    year = row['year']\n",
    "    return eq_weight_port_df.loc[year][0]\n",
    "\n",
    "\n",
    "df_q2_e['ew_annual_return'] = df_q2_e.apply(eq_weight_port_func, axis=1)\n",
    "\n",
    "# Computing the monthly return of the value-weighted portfolio\n",
    "ew_monthly_return_df = df_q2_e[['year', 'month', 'monthly_return']].groupby([\n",
    "                                                                            'year', 'month']).sum()\n",
    "\n",
    "def ew_monthly_return_func(row):\n",
    "    year = row['year']\n",
    "    month = row['month']\n",
    "    return ew_monthly_return_df.loc[(year, month)][0]\n",
    "\n",
    "\n",
    "df_q2_e['ew_monthly_return'] = df_q2_e.apply(ew_monthly_return_func, axis=1)\n",
    "\n",
    "# Computing portfolio statistics\n",
    "ew_min = df_q2_e['ew_annual_return'].min()\n",
    "ew_max = df_q2_e['ew_annual_return'].max()\n",
    "ew_sharperatio = (df_q2_e['ew_AAR'].mean() - risk_free_rate) / \\\n",
    "    df_q2_e['ew_annualized_volatility'].mean()\n",
    "# Showing dataframe\n",
    "df_q2_e[190:196]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_AAR = df_q2_e['ew_AAR'].mean()\n",
    "ew_volatility = df_q2_e['ew_annualized_volatility'].mean()\n",
    "\n",
    "print(f'Equally-weighted portfolio statistics:\\n\\nAAR: {ew_AAR}')\n",
    "print(f'Max yearly return: {ew_max}')\n",
    "print(f'Min yearly return: {ew_min}')\n",
    "print(f'Sharpe ratio: {ew_sharperatio}')\n",
    "print(f'Annualized volatility: {ew_volatility}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.III.III Building Value-weighted Portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building value weighted portfolio with monthly rebalancing\n",
    "df_q2_v = df_q2.copy()\n",
    "df_q2_v.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get monthly total market value\n",
    "val_weight_df = df_q2_v[['year', 'month', 'market_cap']\n",
    "                        ].groupby(['year', 'month']).sum().copy()\n",
    "\n",
    "# function to get value-based weights\n",
    "def val_weight_func(row):\n",
    "    year = row['year']\n",
    "    month = row['month']\n",
    "    return row['market_cap']/val_weight_df.loc[(year, month)][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q2_v['value_weight'] = df_q2_v.apply(val_weight_func, axis=1)\n",
    "df_q2_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the monthly returns for each ISIN based on the market cap weight per month\n",
    "df_q2_v['vw_asset_monthly_weighted_returns'] = df_q2_v['monthly_return'] * \\\n",
    "    df_q2_v['value_weight']\n",
    "\n",
    "# Computing the AAR of the value-weighted portfolio by summing all the monthly weighted returns across the portfolio and dividing by 16 years of data\n",
    "df_q2_v['vw_AAR'] = (df_q2_v['vw_asset_monthly_weighted_returns'].sum())/16\n",
    "\n",
    "# Computing the annual returns of the value-weighted portfolio\n",
    "vw_annual_return_df = df_q2_v[[\n",
    "    'year', 'vw_asset_monthly_weighted_returns']].groupby('year').sum()\n",
    "\n",
    "\n",
    "def vw_annual_return_func(row):\n",
    "    year = row['year']\n",
    "    return vw_annual_return_df.loc[(year)][0]\n",
    "\n",
    "\n",
    "df_q2_v['vw_annual_return'] = df_q2_v.apply(vw_annual_return_func, axis=1)\n",
    "\n",
    "# Computing the monthly return of the value-weighted portfolio\n",
    "vw_monthly_return_df = df_q2_v[['year', 'month', 'vw_asset_monthly_weighted_returns']].groupby([\n",
    "                                                                                               'year', 'month']).sum()\n",
    "\n",
    "\n",
    "def vw_monthly_return_func(row):\n",
    "    year = row['year']\n",
    "    month = row['month']\n",
    "    return vw_monthly_return_df.loc[(year, month)][0]\n",
    "\n",
    "\n",
    "df_q2_v['vw_monthly_return'] = df_q2_v.apply(vw_monthly_return_func, axis=1)\n",
    "\n",
    "# Computing portfolio statistics\n",
    "df_q2_v['vw_annualized_volatility'] = df_q2_v['vw_monthly_return'].std() * \\\n",
    "    math.sqrt(months)\n",
    "vw_min = df_q2_v['vw_annual_return'].min()\n",
    "vw_max = df_q2_v['vw_annual_return'].max()\n",
    "vw_sharperatio = (df_q2_v['vw_annual_return'].mean(\n",
    ") - risk_free_rate)/df_q2_v['vw_annualized_volatility'].mean()\n",
    "# Showing dataframe\n",
    "df_q2_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_AAR = df_q2_v['vw_AAR'].mean()\n",
    "vw_volatility = df_q2_v['vw_annualized_volatility'].mean()\n",
    "\n",
    "print(f'Value-weighted portfolio statistics:\\n\\nAAR: {vw_AAR}')\n",
    "print(f'Max yearly return: {vw_max}')\n",
    "print(f'Min yearly return: {vw_min}')\n",
    "print(f'Sharpe ratio: {vw_sharperatio}')\n",
    "print(f'Annualized volatility: {vw_volatility}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.II.IV Compare the two portfolios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "x = df_q2.groupby('year').year.mean()\n",
    "y1 = df_q2_v.groupby('year').vw_annual_return.mean()\n",
    "y3 = df_q2_e.groupby('year').ew_annual_return.mean()\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add time series traces\n",
    "fig.add_trace(go.Scatter(x=x, y=y1, name='Value-weighted portfolio annual returns',\n",
    "              line=dict(color='lightblue', width=5)))\n",
    "fig.add_trace(go.Scatter(x=x, y=y3, name='Equally-weighted portfolio annual returns',\n",
    "              line=dict(color='darkgreen', width=5)))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Portfolio Performance by Year',\n",
    "                  xaxis_title='Year',\n",
    "                  yaxis_title='Annual Return',\n",
    "                  legend_title='Portfolio Type',\n",
    "                  font=dict(size=16),\n",
    "                  plot_bgcolor='white')\n",
    "\n",
    "# Center legend title\n",
    "fig.update_layout(legend=dict(title=dict(text='Portfolio Type', font=dict(size=18), side='top')),\n",
    "                  legend_title_font=dict(size=18),\n",
    "                  legend_title_side='top')\n",
    "\n",
    "# Center plot title\n",
    "fig.update_layout(title=dict(text='Portfolio Performance by Year',\n",
    "                  font=dict(size=22), x=0.4, xanchor='center'))\n",
    "\n",
    "\n",
    "# Customize axes\n",
    "fig.update_xaxes(tickvals=x,\n",
    "                 ticktext=[str(int(val)) for val in x],\n",
    "                 tickangle=45,\n",
    "                 dtick=1,\n",
    "                 tickfont=dict(size=14),\n",
    "                 gridcolor='lightgray',\n",
    "                 zeroline=False)\n",
    "\n",
    "fig.update_yaxes(tickfont=dict(size=14),\n",
    "                 gridcolor='lightgray',\n",
    "                 zeroline=False)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REMOVE THIS CODE\n",
    "display(df_q2_v)\n",
    "#display(df_q2_e.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Really high numbers in 2009. See https://www.naturalgasintel.com/2009-called-terrific-year-for-energy-investors-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table\n",
    "table = prettytable.PrettyTable()\n",
    "\n",
    "# Add the columns to the table\n",
    "table.field_names = ['Portfolio',\n",
    "                     'Value-weighted portfolio', 'Equally-weighted portfolio']\n",
    "\n",
    "# Add the rows to the table\n",
    "tabledf = {\n",
    "    'Annualized average return': ['Annualized average return', round(vw_AAR, 4), round(ew_AAR, 4)],\n",
    "    'Annualized Volatility': ['Annualized volatility', round(vw_volatility, 4), round(ew_volatility, 4)],\n",
    "    'Minimum return': ['Minimum return', round(vw_min, 4), round(ew_min, 4)],\n",
    "    'Maximum return': ['Maximum return', round(vw_max, 4), round(ew_max, 4)],\n",
    "    'Sharpe Ratio': ['Sharpe Ratio', round(vw_sharperatio, 4), round(ew_sharperatio, 4)]\n",
    "}\n",
    "\n",
    "for row in tabledf:\n",
    "    table.add_row(tabledf[row])\n",
    "\n",
    "# Add borders to the table\n",
    "table.hrules = prettytable.ALL\n",
    "table.header = True\n",
    "table.set_style(prettytable.SINGLE_BORDER)\n",
    "\n",
    "# Save the table to a file\n",
    "with open('value&equal.txt', 'w') as f:\n",
    "    f.write(str(table))\n",
    "\n",
    "# Display the table\n",
    "print(table)\n",
    "\n",
    "table\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.III - For this question, limit your set of firms to 100 randomly selected firms. Pay a particular attention to the construction of the covariance matrix. Build an optimal portfolio with minimum variance with monthly rebalancing over the period 2005-2020. Report the following statistics: annualized average return, annualized volatility, minimum return, maximum return, and Sharpe ratio. Comment on the reported statistics in comparison with the equally-weighted and value-weighted portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.IV - For this question, keep the same randomly selected firms from the previous point. Build an optimal portfolios with various target portfolio returns (e.g., from 2% to 16% with 2% increments). Plot the efficient frontier as well as the individual assets. Which portfolio is the most efficient in terms of Sharpe ratio?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.V - Choose an appropriate benchmark, which corresponds to the region of your dataset. Compare the performance of your portfolios (equally-weighted, value-weighted, and minimum variance) with the benchmark. Comment on the differences.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.VI - Compute and comment on the simple correlation between returns, volatility, size.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.VII - For this question, take the same 100 selected firms. You now create a minimum variance portfolio with monthly rebalancing with an additional constraint: you exclude the smallest firms (bottom tercile of the distribution of the firms’ market capitalization in month t − 1). Report summary statistics on the performance of this portfolio and comment on the differences with the minimum variance from point 3.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
